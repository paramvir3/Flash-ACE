# config.yaml

# ------------------------------------------------------------------------------
# 1. DATASETS
# ------------------------------------------------------------------------------
train_file: "data_2.extxyz"    # Your full dataset
valid_file: null                  # Leave null to use random split
val_split: 0.1                    # 10% of data used for validation (randomly selected)
model_save_path: "model.pt"

# ------------------------------------------------------------------------------
# 2. MODEL ARCHITECTURE
# ------------------------------------------------------------------------------
r_max: 5
l_max: 1
num_radial: 12
hidden_dim: 128
num_layers: 2
radial_basis_type: "bessel"       # Options: bessel, gaussian
radial_trainable: false           # Make radial frequencies/centers learnable
envelope_exponent: 5              # Polynomial cutoff exponent (higher = steeper)
gaussian_width: 0.5              # Relative Gaussian width multiplier (only used if basis=gaussian)

# ------------------------------------------------------------------------------
# 3. TRAINING PARAMETERS
# ------------------------------------------------------------------------------
epochs: 300
batch_size: 8
# Lower start LR for stability; scheduler will reduce further on plateaus
learning_rate: 0.001
# Optional optimizer and scheduler tweaks
weight_decay: 0.0
lr_scheduler_factor: 0.5
lr_scheduler_patience: 8
lr_min: 0.00001
clip_grad_norm: 5.0
use_amp: true                    # Enable mixed precision on CUDA for speed
amp_dtype: "float16"             # Options: float16, bfloat16
grad_accum_steps: 1              # Gradient accumulation steps before optimizer step
precompute_neighbors: true       # Cache neighbor lists to cut CPU overhead
random_rotation: false           # Apply random SO(3) rotations per batch item
device: "cuda"
checkpoint_dir: "checkpoints"     # Directory to stash periodic checkpoints
checkpoint_interval: 0           # Save a checkpoint every N epochs (0 disables)
resume_from: null                # Path to a checkpoint to resume from
resume_load_optimizer: false     # Load optimizer state when resuming
resume_load_scheduler: false     # Load LR scheduler state when resuming
resume_load_scaler: false        # Load AMP GradScaler state when resuming
use_checkpoint_energy_shift: true # Use the energy shift stored in the checkpoint when resuming
energy_shift_per_atom: null       # Constant shift to subtract during training (eV/atom)
atomic_energies: null             # Map of species reference energies, e.g. {H: -0.5, O: -17.0}
solve_atomic_energies: false      # Fit species reference energies from the training set like NequIP/MACE

# ------------------------------------------------------------------------------
# 4. LOSS WEIGHTS
# ------------------------------------------------------------------------------
energy_weight: 1.0
forces_weight: 10.0
stress_weight: 0.1

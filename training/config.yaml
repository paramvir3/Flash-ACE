# config.yaml

# ------------------------------------------------------------------------------
# 1. DATASETS
# ------------------------------------------------------------------------------
train_file: "data_2.extxyz"    # Your full dataset
valid_file: null                  # Leave null to use random split
val_split: 0.1                    # 10% of data used for validation (randomly selected)
model_save_path: "model.pt"

# ------------------------------------------------------------------------------
# 2. MODEL ARCHITECTURE
# ------------------------------------------------------------------------------
r_max: 5
l_max: 1
num_radial: 20             # Slightly wider radial basis for single-layer robustness
hidden_dim: 128
num_layers: 1              # Single lightweight attention block
radial_basis_type: "bessel"       # Options: bessel, gaussian
radial_trainable: false           # Make radial frequencies/centers learnable
envelope_exponent: 5              # Polynomial cutoff exponent (higher = steeper)
gaussian_width: 0.5              # Relative Gaussian width multiplier (only used if basis=gaussian)
local_message_passing: false     # Attention-only path (no local MP)
local_mp_micro_step: false       # Disable micro-steps when using attention-only
local_mp_micro_steps: 0          # No micro-step repeats
local_mp_sharpness: 6.0          # Unused when MP is off
normalize_forces: true           # Standardize forces by dataset RMS (NequIP-style) for stable loss scaling
force_rms: null                  # Optional manual override [fx, fy, fz] eV/Ã…; auto-computed if null

# ------------------------------------------------------------------------------
# 3. TRAINING PARAMETERS
# ------------------------------------------------------------------------------
epochs: 300
batch_size: 8
# Higher force focus with stable LR schedule
learning_rate: 0.001             # Lower LR for deeper attention stability
# Optional optimizer and scheduler tweaks
weight_decay: 0.0
lr_scheduler_factor: 0.5
lr_scheduler_patience: 6         # Faster LR reductions if plateaus
lr_min: 0.00005
clip_grad_norm: 1.0
warmup_steps: 200                # Short LR warmup steps for stability
use_amp: true                    # Enable mixed precision on CUDA for speed
amp_dtype: "bfloat16"            # Prefer bf16 for stability; use float16 if unavailable
grad_accum_steps: 2              # Higher effective batch for steadier forces
precompute_neighbors: true       # Cache neighbor lists to cut CPU overhead
random_rotation: true            # Apply random SO(3) rotations per batch item
edge_dropout_prob: 0.02          # Light dropout to avoid losing too much context in a single block
device: "cuda"
checkpoint_dir: "checkpoints"     # Directory to stash periodic checkpoints
checkpoint_interval: 0           # Save a checkpoint every N epochs (0 disables)
resume_from: null                # Path to a checkpoint to resume from
resume_load_optimizer: false     # Load optimizer state when resuming
resume_load_scheduler: false     # Load LR scheduler state when resuming
resume_load_scaler: false        # Load AMP GradScaler state when resuming
use_checkpoint_energy_shift: true # Use the energy shift stored in the checkpoint when resuming
energy_shift_per_atom: null       # Constant shift to subtract during training (eV/atom)
atomic_energies: null             # Map of species reference energies, e.g. {H: -0.5, O: -17.0}
solve_atomic_energies: true       # Fit species reference energies from the training set (NequIP-style baseline removal)

# ------------------------------------------------------------------------------
# 4. LOSS WEIGHTS
# ------------------------------------------------------------------------------
energy_weight: 0.1               # Small anchor on energies to keep the potential smooth
forces_weight: 12.0              # Slightly softer to pair with the energy anchor
stress_weight: 0.0               # Disable unless stresses are critical

# Auxiliary and regularization weights
aux_force_weight: 0.3            # Stronger auxiliary force supervision
aux_stress_weight: 0.0           # Disable aux stress by default
force_consistency_weight: 0.05   # Mild anchor to keep forces conservative
sobolev_weight: 0.0              # Disable to avoid extra backward passes
sobolev_sigma: 0.0

# Loss/regularization schedules (epoch numbers are 1-indexed)
energy_weight_schedule:
  []
forces_weight_schedule:
  - epoch: 80
    value: 10.0
aux_force_weight_schedule:
  - epoch: 60
    value: 0.1
sobolev_weight_schedule:
  []

# Small-displacement augmentation
displacement_prob: 0.1
displacement_sigma: 0.02

# Attention controls
attention_message_clip: 8.0
attention_conditioned_decay: true
attention_share_qkv: "kv"        # Share KV across heads for stability
temperature_scale_start: 1.4      # Sharper early attention
temperature_scale_end: 0.9
temperature_scale_epochs: 25
temperature_force_ref: 0.5
temperature_force_exponent: -0.25
